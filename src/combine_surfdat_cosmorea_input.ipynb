{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make new zipped input data file with modified surface data and COSMOREA\n",
    "\n",
    "Started 2023-02-28\n",
    "Script by Eva Lieungh, Hui Tang, Elin C.R. Aas, ChatGPT\n",
    "\n",
    "This script takes the modified surface data (created with dataprep_surfacedata notebook) and combines it with COSMOREA data in a new zipped folder that can serve as input to CTSM or the LSP with some local modifications. The cosmorea_readme.md file explains the local changes. \n",
    "\n",
    "As explained in [Hui's readme file](https://github.com/huitang-earth/scripts_ctsm_region/tree/main/atm_forcing/cosmo_rea_6km), COSMA data are available from https://opendata.dwd.de/climate_environment/REA/COSMO_REA6/ and single-site forcing subset from the global data set is already prepared by Hui on Fram and Saga. Elin downloaded a local copy of the VCG site forcing data, which is used here. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import xarray as xr  # NetCDF data handling\n",
    "import netCDF4 \n",
    "import matplotlib.pyplot as plt  # Plotting\n",
    "import time  # Keeping track of runtime\n",
    "import json  # For reading data dictionaries stored in json format\n",
    "import pandas as pd  # Tabular data analysis\n",
    "import datetime as dt  # For workaround with long simulations (beyond year 2262)\n",
    "import statistics as stats # For mean and other calculations\n",
    "from pathlib import Path  # For easy path handling\n",
    "import zipfile # for unzipping\n",
    "import shutil # easiest whole-directory zipping\n",
    "import glob # for wildcard * searching in file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to modified input data made with dataprep_surfacedata.ipynb\n",
    "modified_surfdat_path = Path(f\"C:/Users/evaler/OneDrive - Universitetet i Oslo/Eva/PHD/FATES_INCLINE/data\")\n",
    "\n",
    "# set path to cosmorea files downloaded from Elin\n",
    "cosmorea_path = str(Path(f\"C:/Users/evaler/OneDrive - Universitetet i Oslo/Eva/PHD/FATES_INCLINE/data/VCG/COSMOREA_VCG\"))\n",
    "\n",
    "# set path for where to store finished cosmorea + modified surface data\n",
    "new_inputdata_path = str(Path(f\"C:/Users/evaler/OneDrive - Universitetet i Oslo/Eva/PHD/FATES_INCLINE/data_processed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define LSP site identities and corresponding names \n",
    "siteID = [\"ALP1\",\"ALP2\",\"ALP3\",\"ALP4\",\"SUB1\",\"SUB2\",\"SUB3\",\"SUB4\",\"BOR1\",\"BOR2\",\"BOR3\",\"BOR4\"]\n",
    "siteID1 = [\"Ulvehaugen\",\"Lavisdalen\",\"Gudmedalen\",\"Skjelingahaugen\",\n",
    "           \"Alrust\",\"Hogsete\",\"Rambera\",\"Veskre\",\n",
    "           \"Fauske\",\"Vikesland\",\"Arhelleren\",\"Ovstedalen\"] \n",
    "siteID2 = [\"Ulvehaugen\",\"Lavisdalen\",\"Gudmedalen\",\"Skjellingahaugen\",\n",
    "           \"Alrust\",\"Hogsete\",\"Rambera\",\"Veskre\",\n",
    "           \"Fauske\",\"Vikesland\",\"Arhelleren\",\"Ovstedalen\"]\n",
    "siteID3 = [\"ULV\",\"LAV\",\"GUD\",\"SKJ\",\"ALR\",\"HOG\",\"RAM\",\"VES\",\"FAU\",\"VIK\",\"ARH\",\"OVS\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract surfacedata file from the zipped folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "site:  ALP1 Ulvehaugen\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_ALP1_c221026.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\ALP1\n",
      "-------------------------------------\n",
      "site:  ALP2 Lavisdalen\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_ALP2_c221026.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\ALP2\n",
      "-------------------------------------\n",
      "site:  ALP3 Gudmedalen\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_ALP3_c221027.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\ALP3\n",
      "-------------------------------------\n",
      "site:  ALP4 Skjellingahaugen\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_ALP4_c221027.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\ALP4\n",
      "-------------------------------------\n",
      "site:  SUB1 Alrust\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_SUB1_c221027.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\SUB1\n",
      "-------------------------------------\n",
      "site:  SUB2 Hogsete\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_SUB2_c221027.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\SUB2\n",
      "-------------------------------------\n",
      "site:  SUB3 Rambera\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_SUB3_c221027.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\SUB3\n",
      "-------------------------------------\n",
      "site:  SUB4 Veskre\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_SUB4_c221027.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\SUB4\n",
      "-------------------------------------\n",
      "site:  BOR1 Fauske\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_BOR1_c221027.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\BOR1\n",
      "-------------------------------------\n",
      "site:  BOR2 Vikesland\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_BOR2_c221027.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\BOR2\n",
      "-------------------------------------\n",
      "site:  BOR3 Arhelleren\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_BOR3_c221027.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\BOR3\n",
      "-------------------------------------\n",
      "site:  BOR4 Ovstedalen\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_BOR4_c221027.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\BOR4\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,12):\n",
    "    print(\"-------------------------------------\")\n",
    "    print(\"site: \", siteID[i], siteID2[i])\n",
    "\n",
    "    # Specify the name of the zipped folder, where to extract output to, and what filename pattern to look for \n",
    "    zipped_folder_name = str(modified_surfdat_path / f\"{siteID[i]}.zip\")\n",
    "    extracted_surfacedata_path =  modified_surfdat_path / \"surfacedata\" / siteID[i]\n",
    "    wildcard_filename = \"surfdata*.nc\"\n",
    "\n",
    "    # Open the zip file\n",
    "    with zipfile.ZipFile(zipped_folder_name, \"r\") as zip_file:\n",
    "\n",
    "        # get a list of all the file names in the zip\n",
    "        file_list = zip_file.namelist()\n",
    "\n",
    "        # find the first file that matches the wildcard\n",
    "        matched_file = next((f for f in file_list if glob.fnmatch.fnmatch(f, wildcard_filename)), None)\n",
    "\n",
    "        # if a matching file was found, extract it to the output folder\n",
    "        if matched_file:\n",
    "            zip_file.extract(matched_file, extracted_surfacedata_path)\n",
    "            print(f\"{matched_file} extracted to {extracted_surfacedata_path}\")\n",
    "        else:\n",
    "            print(f\"No matching file found in {zipped_folder_name}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NB! Right now this code might extract the wrong surface data file for ALP1 since that site has two... Check and fix the code!***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine COSMOREA forcing and surfacedata files in new zipped archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surfacedata is here:  C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\n",
      "COSMOREA data is here:  C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\VCG\\COSMOREA_VCG\n"
     ]
    }
   ],
   "source": [
    "print(\"surfacedata is here: \", modified_surfdat_path / \"surfacedata\")\n",
    "print(\"COSMOREA data is here: \", cosmorea_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... manually:\n",
    "\n",
    "1. check that there are no surface files in the COSMO folders already\n",
    "2. copy/move modified surface data into the COSMOREA folders\n",
    "3. then make zip archives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path where new files are stored:  C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data_processed\n",
      "path to files that will be zipped:  C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\VCG\\COSMOREA_VCG\n",
      "-----------------------------------\n",
      "making zipped folder:  ALP1.zip\n",
      "done\n",
      "making zipped folder:  ALP2.zip\n",
      "done\n",
      "making zipped folder:  ALP3.zip\n",
      "done\n",
      "making zipped folder:  ALP4.zip\n",
      "done\n",
      "making zipped folder:  SUB1.zip\n",
      "done\n",
      "making zipped folder:  SUB2.zip\n",
      "done\n",
      "making zipped folder:  SUB3.zip\n",
      "done\n",
      "making zipped folder:  SUB4.zip\n",
      "done\n",
      "making zipped folder:  BOR1.zip\n",
      "done\n",
      "making zipped folder:  BOR2.zip\n",
      "done\n",
      "making zipped folder:  BOR3.zip\n",
      "done\n",
      "making zipped folder:  BOR4.zip\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"path where new files are stored: \", new_inputdata_path)\n",
    "print(\"path to files that will be zipped: \", cosmorea_path)\n",
    "print(\"-----------------------------------\")\n",
    "for i in range(0,12):\n",
    "    print(\"making zipped folder: \", siteID[i] + \".zip\")\n",
    "    folder_to_zip = str(cosmorea_path + \"/\" + siteID[i])\n",
    "    save_zipped_data_here = str(new_inputdata_path + \"/\" + siteID[i])\n",
    "    shutil.make_archive(save_zipped_data_here,\n",
    "                        'zip', \n",
    "                        folder_to_zip)\n",
    "    print(\"done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***(Insert/change code to rename the zipped folders to ALP1-COSMO.zip etc to make them easier to differentiate from default GSWP3!)***\n",
    "\n",
    "Now I manually added \"_cosmo\" to the filename and copied the files back to data together with the other niput files."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then push the changes back to github or some other storage accessible by URL."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ffdbb7dcba71ee9bfd760f02a7add94ffe938549fbb9e2346155ac79cdba3bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
