{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make new zipped input data file with modified surface data and COSMOREA\n",
    "\n",
    "Started 2023-02-28\n",
    "Script by Eva Lieungh, Hui Tang, Elin C.R. Aas, ChatGPT\n",
    "\n",
    "This script takes the modified surface data (created with dataprep_surfacedata notebook) and combines it with COSMOREA data in a new zipped folder that can serve as input to CTSM or the LSP with some local modifications. The cosmorea_readme.md file explains the local changes. \n",
    "\n",
    "As explained in [Hui's readme file](https://github.com/huitang-earth/scripts_ctsm_region/tree/main/atm_forcing/cosmo_rea_6km), COSMA data are available from https://opendata.dwd.de/climate_environment/REA/COSMO_REA6/ and single-site forcing subset from the global data set is already prepared by Hui on Fram and Saga. Elin downloaded a local copy of the VCG site forcing data, which is used here. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import xarray as xr  # NetCDF data handling\n",
    "import netCDF4 \n",
    "import matplotlib.pyplot as plt  # Plotting\n",
    "import time  # Keeping track of runtime\n",
    "import json  # For reading data dictionaries stored in json format\n",
    "import pandas as pd  # Tabular data analysis\n",
    "import datetime as dt  # For workaround with long simulations (beyond year 2262)\n",
    "import statistics as stats # For mean and other calculations\n",
    "from pathlib import Path  # For easy path handling\n",
    "import zipfile # for unzipping\n",
    "import shutil # easiest whole-directory zipping\n",
    "import glob # for wildcard * searching in file names\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to modified input data made with dataprep_surfacedata.ipynb\n",
    "modified_surfdat_path = Path(f\"C:/Users/evaler/OneDrive - Universitetet i Oslo/Eva/PHD/FATES_INCLINE/data\")\n",
    "\n",
    "# set path to cosmorea files downloaded from Elin\n",
    "cosmorea_path = str(Path(f\"C:/Users/evaler/OneDrive - Universitetet i Oslo/Eva/PHD/FATES_INCLINE/data/VCG/COSMOREA_VCG\"))\n",
    "\n",
    "# set path for where to store finished cosmorea + modified surface data\n",
    "new_inputdata_path = str(Path(f\"C:/Users/evaler/OneDrive - Universitetet i Oslo/Eva/PHD/FATES_INCLINE/data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define LSP site identities and corresponding names \n",
    "siteID = [\"ALP1\",\"ALP2\",\"ALP3\",\"ALP4\",\"SUB1\",\"SUB2\",\"SUB3\",\"SUB4\",\"BOR1\",\"BOR2\",\"BOR3\",\"BOR4\"]\n",
    "siteID2 = [\"Ulvehaugen\",\"Lavisdalen\",\"Gudmedalen\",\"Skjellingahaugen\",\n",
    "           \"Alrust\",\"Hogsete\",\"Rambera\",\"Veskre\",\n",
    "           \"Fauske\",\"Vikesland\",\"Arhelleren\",\"Ovstedalen\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract surfacedata file from the zipped folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "site:  ALP1 Ulvehaugen\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_ALP1_c221026.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\ALP1\n",
      "-------------------------------------\n",
      "site:  ALP2 Lavisdalen\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_ALP2_c221026.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\ALP2\n",
      "-------------------------------------\n",
      "site:  ALP3 Gudmedalen\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_ALP3_c221027.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\ALP3\n",
      "-------------------------------------\n",
      "site:  ALP4 Skjellingahaugen\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_ALP4_c221027.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\ALP4\n",
      "-------------------------------------\n",
      "site:  SUB1 Alrust\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_SUB1_c221027.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\SUB1\n",
      "-------------------------------------\n",
      "site:  SUB2 Hogsete\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_SUB2_c221027.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\SUB2\n",
      "-------------------------------------\n",
      "site:  SUB3 Rambera\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_SUB3_c221027.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\SUB3\n",
      "-------------------------------------\n",
      "site:  SUB4 Veskre\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_SUB4_c221027.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\SUB4\n",
      "-------------------------------------\n",
      "site:  BOR1 Fauske\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_BOR1_c221027.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\BOR1\n",
      "-------------------------------------\n",
      "site:  BOR2 Vikesland\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_BOR2_c221027.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\BOR2\n",
      "-------------------------------------\n",
      "site:  BOR3 Arhelleren\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_BOR3_c221027.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\BOR3\n",
      "-------------------------------------\n",
      "site:  BOR4 Ovstedalen\n",
      "surfdata_0.9x1.25_hist_16pfts_Irrig_CMIP6_simyr2000_BOR4_c221027.nc extracted to C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\\BOR4\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,12):\n",
    "    print(\"-------------------------------------\")\n",
    "    print(\"site: \", siteID[i], siteID2[i])\n",
    "\n",
    "    # Specify the name of the zipped folder, where to extract output to, and what filename pattern to look for \n",
    "    zipped_folder_name = str(modified_surfdat_path / f\"{siteID[i]}.zip\")\n",
    "    extracted_surfacedata_path =  modified_surfdat_path / \"surfacedata\" / siteID[i]\n",
    "    wildcard_filename = \"surfdata*.nc\"\n",
    "\n",
    "    # Open the zip file\n",
    "    with zipfile.ZipFile(zipped_folder_name, \"r\") as zip_file:\n",
    "\n",
    "        # get a list of all the file names in the zip\n",
    "        file_list = zip_file.namelist()\n",
    "\n",
    "        # find the first file that matches the wildcard\n",
    "        matched_file = next((f for f in file_list if glob.fnmatch.fnmatch(f, wildcard_filename)), None)\n",
    "\n",
    "        # if a matching file was found, extract it to the output folder\n",
    "        if matched_file:\n",
    "            zip_file.extract(matched_file, extracted_surfacedata_path)\n",
    "            print(f\"{matched_file} extracted to {extracted_surfacedata_path}\")\n",
    "        else:\n",
    "            print(f\"No matching file found in {zipped_folder_name}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NB! Right now this code might extract the wrong surface data file for ALP1 since that site has two... Check and fix the code!***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine COSMOREA forcing and surfacedata files in new zipped archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surfacedata is here:  C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\surfacedata\n",
      "COSMOREA data is here:  C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\VCG\\COSMOREA_VCG\n"
     ]
    }
   ],
   "source": [
    "print(\"surfacedata is here: \", modified_surfdat_path / \"surfacedata\")\n",
    "print(\"COSMOREA data is here: \", cosmorea_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The COSMOREA folders have a different file structure than the LSP expects. In the LSP input files, e.g. ALP2.zip contains folders `datmdata` with GSWP3 forcing .nc files, `user_mods` folder with files \"shell_commands\", \"user_nl_clm\", and \"user_nl_datm_streams\", as well as a domain and surface data file directly in the root folder. \n",
    "\n",
    "Make these folders and move files to get the same structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\evaler\\\\OneDrive - Universitetet i Oslo\\\\Eva\\\\PHD\\\\FATES_INCLINE\\\\src'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(cosmorea_path + \"/\" + \"ALP1/datmdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- ALP1 -----\n",
      "forcing data moved to datmdata\n",
      "----- ALP2 -----\n",
      "forcing data moved to datmdata\n",
      "----- ALP3 -----\n",
      "forcing data moved to datmdata\n",
      "----- ALP4 -----\n",
      "forcing data moved to datmdata\n",
      "----- SUB1 -----\n",
      "forcing data moved to datmdata\n",
      "----- SUB2 -----\n",
      "forcing data moved to datmdata\n",
      "----- SUB3 -----\n",
      "forcing data moved to datmdata\n",
      "----- SUB4 -----\n",
      "forcing data moved to datmdata\n",
      "----- BOR1 -----\n",
      "forcing data moved to datmdata\n",
      "----- BOR2 -----\n",
      "forcing data moved to datmdata\n",
      "----- BOR3 -----\n",
      "forcing data moved to datmdata\n",
      "----- BOR4 -----\n",
      "forcing data moved to datmdata\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,12):\n",
    "    print(\"-----\", siteID[i], \"-----\")\n",
    "    # create datmdata and user_mods directories \n",
    "    if not os.path.exists(cosmorea_path + \"/\" + siteID[i] + \"/datmdata\"):\n",
    "        os.mkdir(cosmorea_path + \"/\" + siteID[i] + \"/datmdata\")\n",
    "    if not os.path.exists(cosmorea_path + \"/\" + siteID[i] + \"/user_mods\"):\n",
    "        os.mkdir(cosmorea_path + \"/\" + siteID[i] + \"/user_mods\")\n",
    "        print(\"created datmdata and user_mods\")\n",
    "\n",
    "    # move files\n",
    "    for filename in os.listdir(cosmorea_path + \"/\" + siteID[i]):\n",
    "        if filename.startswith(\"clm1pt_\"):\n",
    "            source_path = os.path.join(cosmorea_path + \"/\" + siteID[i] + \"/\", filename)\n",
    "            destination_path = os.path.join(cosmorea_path + \"/\" + siteID[i] + \"/datmdata\", filename)\n",
    "            shutil.move(source_path, destination_path)\n",
    "    print(\"forcing data moved to datmdata\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `user_mods` dir is still empty. Copy over the filed from the default forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- ALP1 -----\n",
      "----- ALP2 -----\n",
      "----- ALP3 -----\n",
      "----- ALP4 -----\n",
      "----- SUB1 -----\n",
      "----- SUB2 -----\n",
      "----- SUB3 -----\n",
      "----- SUB4 -----\n",
      "----- BOR1 -----\n",
      "----- BOR2 -----\n",
      "----- BOR3 -----\n",
      "----- BOR4 -----\n"
     ]
    }
   ],
   "source": [
    "GSWP3_default_path = modified_surfdat_path\n",
    "\n",
    "for i in range(0,12):\n",
    "    print(\"-----\", siteID[i], \"-----\")\n",
    "    with zipfile.ZipFile(GSWP3_default_path / f\"{siteID[i]}.zip\", \"r\") as zip_ref:\n",
    "        for file in zip_ref.namelist():\n",
    "            if file.startswith(\"user_mods/\"):\n",
    "                zip_ref.extract(file, cosmorea_path + \"/\" + siteID[i])\n",
    "    print(\"user_mods files successfully moved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a few of the folders and compare with the default GSWP3 data to check that it worked and there are no duplicates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... manually:\n",
    "\n",
    "1. check that there are no surface files in the COSMO folders already\n",
    "2. copy/move modified surface data into the COSMOREA folders\n",
    "3. then make zip archives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path where new files are stored:  C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data_processed\n",
      "path to files that will be zipped:  C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\VCG\\COSMOREA_VCG\n",
      "-----------------------------------\n",
      "making zipped folder:  ALP1_cosmorea.zip\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"path where new files are stored: \", new_inputdata_path)\n",
    "print(\"path to files that will be zipped: \", cosmorea_path)\n",
    "print(\"-----------------------------------\")\n",
    "for i in range(0,1):\n",
    "    print(\"making zipped folder: \", siteID[i] + \"_cosmorea.zip\")\n",
    "    folder_to_zip = str(cosmorea_path + \"/\" + siteID[i])\n",
    "    save_zipped_data_here = str(new_inputdata_path + \"/\" + siteID[i] + \"_cosmorea\")\n",
    "    shutil.make_archive(save_zipped_data_here,\n",
    "                        'zip', \n",
    "                        folder_to_zip)\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path where new files are stored:  C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\n",
      "path to files that will be zipped:  C:\\Users\\evaler\\OneDrive - Universitetet i Oslo\\Eva\\PHD\\FATES_INCLINE\\data\\VCG\\COSMOREA_VCG\n",
      "-----------------------------------\n",
      "making zipped folder:  ALP1_cosmorea.zip\n",
      "done\n",
      "making zipped folder:  ALP2_cosmorea.zip\n",
      "done\n",
      "making zipped folder:  ALP3_cosmorea.zip\n",
      "done\n",
      "making zipped folder:  ALP4_cosmorea.zip\n",
      "done\n",
      "making zipped folder:  SUB1_cosmorea.zip\n",
      "done\n",
      "making zipped folder:  SUB2_cosmorea.zip\n",
      "done\n",
      "making zipped folder:  SUB3_cosmorea.zip\n",
      "done\n",
      "making zipped folder:  SUB4_cosmorea.zip\n",
      "done\n",
      "making zipped folder:  BOR1_cosmorea.zip\n",
      "done\n",
      "making zipped folder:  BOR2_cosmorea.zip\n",
      "done\n",
      "making zipped folder:  BOR3_cosmorea.zip\n",
      "done\n",
      "making zipped folder:  BOR4_cosmorea.zip\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"path where new files are stored: \", new_inputdata_path)\n",
    "print(\"path to files that will be zipped: \", cosmorea_path)\n",
    "print(\"-----------------------------------\")\n",
    "for i in range(0,12):\n",
    "    print(\"making zipped folder: \", siteID[i] + \"_cosmorea.zip\")\n",
    "    folder_to_zip = str(cosmorea_path + \"/\" + siteID[i])\n",
    "    save_zipped_data_here = str(new_inputdata_path + \"/\" + siteID[i] + \"_cosmorea\")\n",
    "    shutil.make_archive(save_zipped_data_here,\n",
    "                        'zip', \n",
    "                        folder_to_zip)\n",
    "    print(\"done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then push the changes back to github or some other storage accessible by URL."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ffdbb7dcba71ee9bfd760f02a7add94ffe938549fbb9e2346155ac79cdba3bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
